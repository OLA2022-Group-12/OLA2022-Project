{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae2163bd",
   "metadata": {},
   "source": [
    "# Stupid Learner Notebook\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In the following notebook we will analyze the behaviour of a particular learner type called *Stupid Learner*.\n",
    "\n",
    "The name derives from the fact that it doesn't actually learn anything during each iteration of the simulation but it always subdivides the current total budget equally among the various products instead.\n",
    "\n",
    "### Walkthrough\n",
    "\n",
    "Since the learner doesn't change over time and its decision algorithm is quite easy we are going to vary the environmental conditions and observe how the \"*constant prediction algorithm*\" scores in different situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd0f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ola2022_project.environment.environment import example_environment, Step\n",
    "from ola2022_project.simulation.simulation import Simulation \n",
    "from ola2022_project.learners.stupid_learner import StupidLearner # TODO: Fix import strategy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1e84832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used for quickly plotting an experiment onto a graph and showing it\n",
    "def plot_experiment(n_days, rewards_per_experiment):\n",
    "    # Calculating and shaping data that is going to be shown on the graph\n",
    "    rewards = rewards_per_experiment\n",
    "    days = np.arange(1, n_days + 1, 1)\n",
    "    mean = [np.mean(rewards_per_experiment)] * n_days\n",
    "    # Creating a new figure and plotting the data onto it\n",
    "    plt.figure()\n",
    "    plt.plot(days, rewards, label = \"Experiment\", marker = \".\", linestyle = '-')\n",
    "    plt.plot(days, mean, label = 'Mean', linestyle = '--')\n",
    "    # Setting labels and showing the figure\n",
    "    plt.xlabel(\"days\")\n",
    "    plt.ylabel(\"reward\")\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13147ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Number Generator used as a source of randomness by the environment and the simulation\n",
    "rng = np.random.default_rng()\n",
    "# Arbitrary total budget and product prices\n",
    "total_budget = 100\n",
    "product_prices = [5, 25, 10, 15, 9]\n",
    "# Environment containing all of the contextual information\n",
    "env = example_environment(\n",
    "    rng = rng, \n",
    "    total_budget = total_budget, \n",
    "    product_prices = product_prices\n",
    ")\n",
    "# Simulation parameters\n",
    "n_experiments = 1\n",
    "n_days = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1512af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02474cea35e24effb77b15e9fa271b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "days:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot cast array data from dtype('O') to dtype('float64') according to the rule 'safe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m simulation \u001b[38;5;241m=\u001b[39m Simulation(\n\u001b[1;32m      3\u001b[0m     rng,\n\u001b[1;32m      4\u001b[0m     env,\n\u001b[1;32m      5\u001b[0m     step \u001b[38;5;241m=\u001b[39m Step\u001b[38;5;241m.\u001b[39mZERO\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Running the simulation\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m simulation\u001b[38;5;241m.\u001b[39msimulate(n_days)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Plotting the experiment\u001b[39;00m\n\u001b[1;32m     12\u001b[0m plot_experiment(n_days, simulation\u001b[38;5;241m.\u001b[39mrewards)\n",
      "File \u001b[0;32m~/Documents/OLA/OLA2022-Project/ola2022_project/simulation/simulation.py:169\u001b[0m, in \u001b[0;36mSimulation.simulate\u001b[0;34m(self, n_days, show_progress_graphs)\u001b[0m\n\u001b[1;32m    166\u001b[0m budgets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearner\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasked_env)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Compute interactions for the entire day\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m interactions \u001b[38;5;241m=\u001b[39m \u001b[43mget_day_of_interactions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpopulation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbudgets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, interactions)\n\u001b[1;32m    173\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInteractions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minteractions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/OLA/OLA2022-Project/ola2022_project/environment/environment.py:435\u001b[0m, in \u001b[0;36mget_day_of_interactions\u001b[0;34m(rng, population, budgets, env_data, de_noise)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# Replace ratios that are 0 with machine-espilon (10^-16) to ensure\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# compatibility with the Dirichlet function\u001b[39;00m\n\u001b[1;32m    433\u001b[0m alpha_ratios \u001b[38;5;241m=\u001b[39m replace_zeros(alpha_ratios)\n\u001b[0;32m--> 435\u001b[0m alpha_ratios_noisy \u001b[38;5;241m=\u001b[39m \u001b[43mrng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirichlet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha_ratios\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mde_noise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m users_landing_on_pages \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrint(\n\u001b[1;32m    437\u001b[0m     np\u001b[38;5;241m.\u001b[39mdelete(alpha_ratios_noisy, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m class_population\n\u001b[1;32m    438\u001b[0m )\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    440\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirichlet output (doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt include competitor): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha_ratios_noisy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    442\u001b[0m )\n",
      "File \u001b[0;32m_generator.pyx:4308\u001b[0m, in \u001b[0;36mnumpy.random._generator.Generator.dirichlet\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot cast array data from dtype('O') to dtype('float64') according to the rule 'safe'"
     ]
    }
   ],
   "source": [
    "# Creating the simulation\n",
    "simulation = Simulation(\n",
    "    rng,\n",
    "    env,\n",
    "    step = Step.ZERO\n",
    ")\n",
    "\n",
    "# Running the simulation\n",
    "simulation.simulate(n_days)\n",
    "\n",
    "# Plotting the experiment\n",
    "plot_experiment(n_days, simulation.rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e965c73",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "As expected, the reward obtained by the learner fluctuates over time without any particular trend since we are making a constant prediction in a random environment.\n",
    "\n",
    "The variance of the rewards is high and there is little to no correlation between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d21e41",
   "metadata": {},
   "source": [
    "# Warning [TODO]\n",
    "All cells below are deprecated due the environment rework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53172353",
   "metadata": {},
   "source": [
    "### Another scenario\n",
    "\n",
    "What if we take into consideration an enviroment where there is a great imbalance in the products' prices?\n",
    "\n",
    "Intuitively we would expect that the reward is lower since, each day, a fixed amount of the total budget gets \"wasted\" on the products with high prices that are way beyond the reservation prices of most customers.\n",
    "\n",
    "This would be true for all learners but it is more evident for this particular type of learner since it doesn't try to adapt to the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae1993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a new environment\n",
    "#new_product_prices = [5, 1, 80, 55, 8]\n",
    "#new_env = example_environment(\n",
    "#    rng = rng, \n",
    "#    total_budget = total_budget, \n",
    "#    product_prices = new_product_prices\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b178961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the new simulation\n",
    "#simulation = Simulation(\n",
    "#    rng,\n",
    "#    env,\n",
    "#    step = Step.ZERO\n",
    "#)\n",
    "\n",
    "# Running the new simulation\n",
    "#simulation.simulate(n_days)\n",
    "\n",
    "# Plotting the experiment\n",
    "#plot_experiment(n_days, simulation.rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e7fc2e",
   "metadata": {},
   "source": [
    "### Results \n",
    "\n",
    "We can see that the generated data supports our hypotesis and, in addition, we can even show that the increase in rewards generated by incrementing the total budget is smaller when the prices are imbalanced due to the reasons stated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8762cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_budgets = 6\n",
    "#budget_interval = 25\n",
    "#inc_env = inc_new_env = np.array([])\n",
    "#inc_rewards_per_experiment = inc_new_rewards_per_experiment = np.empty((0, n_days))\n",
    "# Creating the previous two environments with various total budgets\n",
    "#for i in range(0, n_budgets):\n",
    "#    inc_env = np.append(inc_env, example_environment(\n",
    "#        rng = rng, \n",
    "#        total_budget = total_budget + (i * budget_interval), \n",
    "#        product_prices = product_prices\n",
    "#    ))\n",
    "#    inc_new_env = np.append(inc_new_env, example_environment(\n",
    "#        rng = rng, \n",
    "#        total_budget = total_budget + (i * budget_interval), \n",
    "#        product_prices = new_product_prices\n",
    "#    ))\n",
    "# Running their respective simulations\n",
    "#for i in range(0, n_budgets):\n",
    "#    inc_rewards_per_experiment = np.vstack([inc_rewards_per_experiment, simulation(\n",
    "#        rng,\n",
    "#        inc_env[i],\n",
    "#        learner_factory = StupidLearner,\n",
    "#        n_experiment = n_experiments,\n",
    "#        n_days = n_days,\n",
    "#    )])\n",
    "#    inc_new_rewards_per_experiment = np.vstack([inc_new_rewards_per_experiment, simulation(\n",
    "#        rng,\n",
    "#        inc_new_env[i],\n",
    "#        learner_factory = StupidLearner,\n",
    "#        n_experiment = n_experiments,\n",
    "#        n_days = n_days,\n",
    "#    )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b54f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the comparison between the two averages in both cases\n",
    "#budgets = [total_budget + i * budget_interval for i in range(0, n_budgets)]\n",
    "#means = [np.mean(inc_rewards_per_experiment[i]) for i in range(0, n_budgets)]\n",
    "#new_means = [np.mean(inc_new_rewards_per_experiment[i]) for i in range(0, n_budgets)]\n",
    "#plt.figure()\n",
    "#plt.plot(budgets, means, label = \"Balanced prices\", marker = \".\", linestyle = '-')\n",
    "#plt.plot(budgets, new_means, label = \"Imbalanced prices\", marker = \".\", linestyle = '-')\n",
    "#plt.xlabel(\"budgets\")\n",
    "#plt.ylabel(\"mean reward\")\n",
    "#plt.legend(loc = \"upper right\")\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
